{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e06904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.2.0.tar.gz (281.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 281.3 MB 42 kB/s  eta 0:00:01    |█████                           | 44.5 MB 6.5 MB/s eta 0:00:37\n",
      "\u001b[?25hCollecting py4j==0.10.9.2\n",
      "  Downloading py4j-0.10.9.2-py2.py3-none-any.whl (198 kB)\n",
      "\u001b[K     |████████████████████████████████| 198 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.0-py2.py3-none-any.whl size=281805912 sha256=ca12b356c606bca02ea01f7912954fa83cb4801cac3779b25c0bd14de41ccf04\n",
      "  Stored in directory: /home/olakt/.cache/pip/wheels/2f/f8/95/2ad14a4614b4a9f645ee928fbbd057b1b254c67adb494c9a58\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.2 pyspark-3.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92934fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/30 19:23:26 WARN Utils: Your hostname, pop-os resolves to a loopback address: 127.0.1.1; using 192.168.0.14 instead (on interface wlo1)\n",
      "21/12/30 19:23:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/olakt/.local/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/olakt/.ivy2/cache\n",
      "The jars for the packages stored in: /home/olakt/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-01cf9bc4-35e1-48e6-8d0a-dc19bc300555;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/olakt/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;3.0.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;4.0.5 in central\n",
      "\tfound org.mongodb#bson;4.0.5 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;4.0.5 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/3.0.1/mongo-spark-connector_2.12-3.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;3.0.1!mongo-spark-connector_2.12.jar (66ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/4.0.5/mongodb-driver-sync-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;4.0.5!mongodb-driver-sync.jar (19ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/4.0.5/bson-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;4.0.5!bson.jar (38ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/4.0.5/mongodb-driver-core-4.0.5.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;4.0.5!mongodb-driver-core.jar (118ms)\n",
      ":: resolution report :: resolve 814ms :: artifacts dl 248ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;4.0.5 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;4.0.5 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;3.0.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   4   |   4   |   0   ||   4   |   4   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-01cf9bc4-35e1-48e6-8d0a-dc19bc300555\n",
      "\tconfs: [default]\n",
      "\t4 artifacts copied, 0 already retrieved (2728kB/8ms)\n",
      "21/12/30 19:23:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/home/olakt/.local/lib/python3.9/site-packages/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- oid: string (nullable = true)\n",
      " |-- actual_prom: integer (nullable = true)\n",
      " |-- admission_year: integer (nullable = true)\n",
      " |-- arrival_prom: integer (nullable = true)\n",
      " |-- attendances: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- campus: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- dropped: boolean (nullable = true)\n",
      " |-- dropped_prom: integer (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- graduated: boolean (nullable = true)\n",
      " |-- graduation_year: integer (nullable = true)\n",
      " |-- id: integer (nullable = true)\n",
      " |-- internship: integer (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- number: string (nullable = true)\n",
      " |-- status: struct (nullable = true)\n",
      " |    |-- hired: boolean (nullable = true)\n",
      " |    |-- field: string (nullable = true)\n",
      " |    |-- monthsBfrJob: integer (nullable = true)\n",
      " |-- successfullness: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "\n",
    "def init_spark():\n",
    "    sql = SparkSession.builder \\\n",
    "        .appName(\"trip-app\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:3.0.1\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://root:example@localhost:27017/5data.students\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://root:example@localhost:27017/5data.spark\") \\\n",
    "        .getOrCreate()\n",
    "    sc = sql.sparkContext\n",
    "    return sql, sc\n",
    "\n",
    "\n",
    "sql, sc = init_spark()\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df = sql.read.format(\"com.mongodb.spark.sql.DefaultSource\").option(\"uri\",\n",
    "                                                                   \"mongodb://root:example@localhost:27017/5data.students?authSource=admin\").load()\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"students\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1316c",
   "metadata": {},
   "source": [
    "## The following code is some exemple of calcul we can do after connection spark with   mongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80d1cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_by_year():\n",
    "    connection = \"mongodb://root:example@localhost:27017/spark.student_by_year?authSource=admin\"\n",
    "    result_data = sqlContext.sql(\"SELECT COUNT(*) as students, admission_year from students GROUP BY admission_year\")\n",
    "    result_data.write.format(\"mongo\") \\\n",
    "        .option(\"uri\", connection) \\\n",
    "        .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "284c658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_by_year()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "739b9c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.9 are installed in '/home/olakt/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed numpy-1.21.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2504bbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.5 MB 5.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /home/olakt/.local/lib/python3.9/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/lib/python3/dist-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/olakt/.local/lib/python3.9/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "Successfully installed pandas-1.3.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec35de06",
   "metadata": {},
   "source": [
    "## Here we dig a little bit more in python using spark and some python library for Machine learning ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aa12d1",
   "metadata": {},
   "source": [
    "### Here we take a quick look at the average timebefore student graduation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c237f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7b2498c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_time_graduation():\n",
    "    connection = \"mongodb://root:example@localhost:27017/5data.students?authSource=admin\"\n",
    "    data_obj = sqlContext.sql(\"SELECT avg(status.monthsBfrJob) as graduation_time from students WHERE status.hired == 1\")\n",
    "    df = data_obj.toPandas()\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6d0af4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>graduation_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.944251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   graduation_time\n",
       "0        11.944251"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mean_time_graduation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ad7ad1",
   "metadata": {},
   "source": [
    "### Here we create a collection to show the regions with more contrat pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff5b26d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "31e0d3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_contrat_pro():\n",
    "    connection = \"mongodb://root:example@localhost:27017/spark.more_contrat_pro?authSource=admin\"\n",
    "    result_data = sqlContext.sql(\"SELECT COUNT(_id) as number_of_cp, campus from students WHERE internship != 0 GROUP BY campus ORDER BY number_of_cp DESC\")\n",
    "    result_data.write.format(\"mongo\") \\\n",
    "        .option(\"uri\", connection) \\\n",
    "        .mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79326b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "more_contrat_pro()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dd71b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
