{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d41b274c",
   "metadata": {},
   "source": [
    "# Guide Procédural pour le projet de 5 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7240dd6",
   "metadata": {},
   "source": [
    "Il s'agit d'un proof of concept de ce fait notre projet ne se base pas sur l'implémentaion total des fonctionnalité demandés, mais sut leur faisabilité."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d949e",
   "metadata": {},
   "source": [
    "## Génération de la base de donné"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410157ed",
   "metadata": {},
   "source": [
    "N'ayant pas de base de données à notre disposition nous somme partis sur la conception d'une base de données from scratch utilisant un script node js.\n",
    " - Le script data.js est disponible sur la repository github mis à disposition lors du rendu de notre travail\n",
    " - Pour la réalisation, nous avons utiliser le module Faker io\n",
    " - Le resulat de notre script retourne un ensemble d'objet json représentant les documents qui vont peupler une future collection dites students"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d490ea67",
   "metadata": {},
   "source": [
    "## Choix de la DataBase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea851a3",
   "metadata": {},
   "source": [
    "Comme vu lors du cours, nous avons opter pour une base de données no SQL en raison de la haute flexibilité et de l'évolutivité de ce genre de base de données.\n",
    "\n",
    "Notre choix s'est porté sur Mongo DB. Pour plusieurs raisons:\n",
    "\n",
    "- Champs de connaissances des membres du groupe plus elargi des membre du groupe sur les technologies Mongo\n",
    "- Faciliter d'importer des données sous le formats json à partir de Mongo\n",
    "- Possibilité de faire un Mongo Réplicat afin d'assurer la haute disponibilité des données dans notre architecture\n",
    "- La base de données dont nous avons besoin est plutôt simple et pas besoin d'hierrarchie\n",
    "- Mongo est une base de données CP donc tolérente au partitionnement et reste cohérente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ea99eb",
   "metadata": {},
   "source": [
    "## Type de traitements effectués"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c5ea3a",
   "metadata": {},
   "source": [
    "Il nous ai demandé de répondre à divers problématique sur la base des données collectées. Etant dans un context de Big data processing, nous avons choisi d'utiliser SPARK, pour réaliser les divers traitements en raison des nombreux avantages conférés par spark:\n",
    "\n",
    "Apache Spark est l’outil de génération de données massives largement utilisé par les industries  Apache Spark atteint des performances élevées pour les données par lots et en continu, en utilisant un planificateur DAG de pointe, un optimiseur de requêtes et un moteur d’exécution physique. Spark est principalement conçu pour la science des données et les abstractions de Spark facilitent les choses. Apache Spark fournit des API de haut niveau en Java, Scala, Python et R. Il dispose également d’un moteur optimisé pour les graphes d’exécution généraux. En traitement de données, Apache Spark est le plus grand projet open source.\n",
    "\n",
    "- Calcul en mémoire dans Spark\n",
    "\n",
    "Avec le traitement en mémoire, nous pouvons augmenter la vitesse de traitement. Ici, les données sont mises en cache, nous n’avons donc pas besoin d’extraire les données du disque, le temps est donc sauvegardé. Spark dispose d’un moteur d’exécution DAG qui facilite le calcul en mémoire et le flux de données acyclique se traduisant par une vitesse élevée.\n",
    "\n",
    "- Rapidité de Traitement\n",
    "\n",
    "Avec Apache Spark, on atteint une vitesse de traitement de données élevée. Environ 100 fois plus rapide en mémoire et 10 fois plus rapide sur le disque. Ceci est rendu possible en réduisant le nombre de lecture-écriture sur le disque.\n",
    "\n",
    "- Dynamique dans la nature\n",
    "\n",
    "Il est facilement possible de développer une application parallèle, car Spark fournit 80 opérateurs de haut niveau.\n",
    "Tolérance aux pannes dans l’étincelle\n",
    "\n",
    "Apache Spark offre une tolérance aux pannes via Spark abstraction-RDD. Les RDD Spark sont conçus pour gérer l’échec de tout nœud de travail du cluster. Ainsi, cela garantit une perte de données nulle.\n",
    "\n",
    "**De plus, pour effectuer les calcul et les traitements grace à Spark nous allons utiliser python et plus précisement pyspark pour interargir avec spark et effectuer les divers actions de requêtage et de calcul prévisionnel**\n",
    "\n",
    "Notons aussi l'utilisation des libraries comme numpy, pandas, ainsi que des contexts SQL nécessitant l'utilisation du Language SQL pour interagir avec notre base de données Mongo DB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66687f34",
   "metadata": {},
   "source": [
    "## Architecture du Système"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034226a2",
   "metadata": {},
   "source": [
    "Notre Architecture entière tourne sur docker. A l'aide de divers dockerfile, nous avons mis en place trois principaux services:\n",
    "- service spark : comportant 2 workers et disponible au http://localhost:9090/\n",
    "- service mongo : pouvant contenir un nombre n de replication et disponible sur http://localhost:8000/\n",
    "- service mongo chart: qui va nous servir de web interface pour afficher les différents graphes issu de l'anlyse descriptive de notre data base et disponible au http://localhost:80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c6e7e",
   "metadata": {},
   "source": [
    "Grâce à la mise en place d'un docker swarm, les différents service déployé sont hautement disponible et résiliant à la faute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23889a",
   "metadata": {},
   "source": [
    "Vous Trouverez tous les schémas et les code source des divers services dans la doc fournie avec notre projet et sur la repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e7a221",
   "metadata": {},
   "source": [
    "## Réponse au Proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e743ad78",
   "metadata": {},
   "source": [
    "Nous sommes objectif et categorique, la réalisation d'un projet comme présenté dans le sujet de notre devoir est largement possible sur la base de l'architecture que nous avons mis en place.\n",
    "\n",
    "pyspark et python en général nous permettent de faire des extraire des données sur mongo sans limitation de charge ou de temps de calcul.\n",
    "\n",
    "Grâce aussi aux divers librairie de machine Learning et de data manipulation fournies avec python, nous pouvons comme nous le monterons au cours de notre démo créer de nouvelles collections dédiées à repondre à divers questions.\n",
    "\n",
    "Ensuite, pour ce qui concerne le display, mongo chart grâce à ses fonctions SQL intégrer facilite grandement la visualisation de nos données et avec une interface utilisateur facile à prendre en main.\n",
    "\n",
    "Enfin, l'architecture global étant dynamique et automatisé, elle permettra le rafraîchissement automatique des graphes lors du changement ou de la modification de la base de données mongo.\n",
    "\n",
    "Pour preuve, au cours de la réalisation de ce projet, nos calculs ont été réalisés avec environ 1.000.000 d'étudiants, ce qui se rapproche des performance requis en matière de big data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6b175",
   "metadata": {},
   "source": [
    "# Nous vous remercions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
